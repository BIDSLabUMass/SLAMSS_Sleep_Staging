{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import glob\n",
    "from torchvision.datasets.folder import IMG_EXTENSIONS\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 200\n",
    "BATCH_SIZEV = 200\n",
    "BATCH_SIZEV2 = 600\n",
    "EPOCH = 500\n",
    "w = 3\n",
    "in_w = 3\n",
    "#\n",
    "sqe = 12\n",
    "# # of class\n",
    "cls = 4\n",
    "kk= 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "    \n",
    "        self.cnn = nn.Sequential( \n",
    "        nn.Conv1d(in_w, 64, 9, stride=1, padding=4),\n",
    "        nn.LeakyReLU(inplace=False),\n",
    "        nn.Conv1d(64, 64, 9, stride=1, padding=4),\n",
    "        nn.LeakyReLU(inplace=False),\n",
    "        nn.Conv1d(64, 64, 9, stride=1, padding=4),\n",
    "        #nn.LeakyReLU(inplace=False), #put it back 2020 706\n",
    "        )\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        \n",
    "        #embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src sent len, batch size, emb dim]\n",
    "\n",
    "        src1 = src.permute(0, 2, 1)\n",
    "        #print src1.size()\n",
    "        src2 = self.cnn(src1)\n",
    "        #print src2.size()\n",
    "        src3 = src2.permute(0, 2, 1)\n",
    "        #print src3.size()\n",
    "\n",
    "    \n",
    "    \n",
    "        return src3\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 1) + dec_hid_dim, dec_hid_dim, bias=True)\n",
    "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        \n",
    "\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        #repeat encoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        #encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src sent len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        #print energy\n",
    "        #energy = [batch size, src sent len, dec hid dim]\n",
    "        \n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        \n",
    "        #energy = [batch size, dec hid dim, src sent len]\n",
    "        \n",
    "        #v = [dec hid dim]\n",
    "        \n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
    "        \n",
    "        #v = [batch size, 1, dec hid dim]\n",
    "     \n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        \n",
    "        #attention= [batch size, src len]\n",
    "\n",
    "        \n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "        #ORINAL 1 # 5\n",
    "        self.rnn = nn.GRU(64, enc_hid_dim, 3, bidirectional = False, dropout=0.5, batch_first = True)\n",
    "        \n",
    "        self.fc = nn.Sequential( \n",
    "            nn.Linear(enc_hid_dim * 1, dec_hid_dim, bias=True),\n",
    "            #nn.BatchNorm1d(dec_hid_dim),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "     \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        \n",
    "\n",
    "        outputs, hidden = self.rnn(src)\n",
    "  \n",
    "        #hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        hidden = self.fc(hidden[-1,:,:])\n",
    "        #hidden = self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "\n",
    "        #outputs = [src sent len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        #print outputs\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        #self.dropout = dropout\n",
    "        self.attention = attention\n",
    "        \n",
    "        #self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 1) + emb_dim, dec_hid_dim,batch_first = True, dropout=0.0)\n",
    "        \n",
    "        self.out = nn.Sequential( \n",
    "\n",
    "            nn.Linear((enc_hid_dim * 1) + dec_hid_dim + emb_dim, output_dim, bias=False),\n",
    "            #nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs, trans):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        #print a        \n",
    "        #a = [batch size, src len]\n",
    "    \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        encoder_outputs = encoder_outputs.permute(0, 1, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "\n",
    "        rnn_input = torch.cat((input, weighted), dim = 2)\n",
    "        rnn_input = rnn_input.permute(1, 0, 2)\n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "        #print hidden.size()\n",
    "\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        #print output \n",
    "        #output = [sent len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "\n",
    "        #assert (output == hidden).all()\n",
    "        \n",
    "        #input = input.squeeze(0)\n",
    "        output = output.squeeze(1)\n",
    "        output = output.unsqueeze(0)\n",
    "        #weighted = weighted.squeeze(0)\n",
    "        #print(output.size())\n",
    "        #print(weighted.size())\n",
    "        #print(input.size())\n",
    "        #print(trans.size())\n",
    "        #trans = trans.unsqueeze(0)\n",
    "        #output = self.out(torch.cat((output, weighted, input, trans), dim = 2))\n",
    "        output = self.out(torch.cat((output, weighted, input), dim = 2))\n",
    "        #output = [bsz, output dim]\n",
    "        \n",
    "        return output, hidden.squeeze(0)\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device = 0):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        tn = scipy.io.loadmat(\"./transition.mat\")\n",
    "        trans = tn['transition_train_raw']\n",
    "        self.trans = Variable(torch.FloatTensor(trans).cuda())\n",
    "        #print(trans)\n",
    "        #assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "        #    \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        #assert encoder.n_layers == decoder.n_layers, \\\n",
    "        #    \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        max_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, max_len-1, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        #print hidden.size()\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[:,0]\n",
    "        _, ind = torch.max(input,1)\n",
    "        trans_bat = torch.zeros(batch_size, 5).to(self.device)\n",
    "        for i in range(batch_size):\n",
    "            trans_bat[i,:] = self.trans[ind[i],:]\n",
    "        #print(input.size())\n",
    "        #print(input2.size())\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs, trans_bat)\n",
    "\n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "   \n",
    "            if output.size()[0] == 1:\n",
    "                outputs[:,t-1,:] = output[:,:]\n",
    "            else:\t\n",
    "                outputs[:,t-1,:] = output[:,0,:]\n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            #need to chekc again   tzuan\n",
    "            top1 = output.argmax(1)\n",
    "            top1 = top1.view(-1,1).type(torch.cuda.FloatTensor)\n",
    "            #print \"Tp: \", top1.size()\n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[:,t] if teacher_force else output.squeeze(0)\n",
    "        \n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class features_dataset(Data.Dataset):\n",
    "\n",
    "    def __init__(self, mode, p):\n",
    "\n",
    "        self.mode = mode\n",
    "        self.data = glob.glob(p)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "\n",
    "        if self.mode == 0:\n",
    "            data = scipy.io.loadmat(self.data[index])\n",
    "            x = np.squeeze(data['sqe_x'])[:800,:]\n",
    "            y = data['sqe_y'][:800,:]\n",
    "        elif self.mode == 1:\n",
    "            data = scipy.io.loadmat(self.data[index])\n",
    "            x = np.squeeze(data['sqe_x'])\n",
    "            y = data['sqe_y']\n",
    "            up_lim = x.shape[0] - 700\n",
    "            ind = random.randint(sqe, up_lim)\n",
    "            tmp_x = x[(ind-sqe):ind+700,:]\n",
    "            tmp_y = y[(ind-sqe):ind+700,:]\n",
    "            x = tmp_x#.reshape((sqe+50, w)) + np.random.normal(0,0.000001, size=(50+sqe, w))\n",
    "            y = tmp_y\n",
    "\n",
    "        else:\n",
    "            data = scipy.io.loadmat(self.data[index])\n",
    "            x = np.squeeze(data['sqe_x'])\n",
    "            y = data['sqe_y']\n",
    "            z = data['p_info']\n",
    "            tem_x = np.zeros((2953,4))\n",
    "            tem_y = np.zeros((2953,1))\n",
    "            tem_x[:x.shape[0],:] = x[:,[0,1,6,7]]\n",
    "            tem_y[:y.shape[0],:] = y[:,:]\n",
    "            x1 = torch.FloatTensor(tem_x) \n",
    "            y1 = torch.LongTensor(tem_y)\n",
    "            #y2 = torch.zeros(x1.size()[0],4).scatter_(dim=1, index=y1, src=torch.tensor(1.0))\n",
    "            y2 = torch.zeros(x1.size()[0],4).scatter_(1, y1, 1)\n",
    "            return x1, y1, y2, x.shape[0],z\n",
    "        #x = self.input_x[index].reshape((h, w))\n",
    "        #y = self.input_y[index]\t\t\t\n",
    "        #x = x[:,[0, 1, 6]]\t\n",
    "        x = x[:,[1,6,7]]\n",
    "        x1 = torch.FloatTensor(x) \n",
    "        y1 = torch.LongTensor(y)#.unsqueeze(1)\n",
    "\n",
    "        #print(y1.size(), \"                    \"  ,x1.size())\n",
    "\n",
    "        #print y1.size()\n",
    "        #y2 = torch.zeros(x1.size()[0],4).scatter_(dim=1, index=y1, src=torch.tensor(1.0))\n",
    "        y2 = torch.zeros(x1.size()[0],4).scatter_(1, y1, 1)\n",
    "        #print y2.size()\n",
    "        #quit()\n",
    "\n",
    "        return x1, y1, y2\n",
    "    def __len__(self):\n",
    "        if self.mode == 1:\n",
    "            return(len(self.data))\n",
    "        else:\n",
    "            return(len(self.data))\n",
    "            #return(len(self.x))\n",
    "    def getName(self):\n",
    "        return self.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_d = \"./data_info_v/*.mat\"\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    features_dataset(mode = 2, p=path_d),\n",
    "    batch_size = BATCH_SIZEV,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded!\n",
      "CNN(\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv1d(3, 64, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "  )\n",
      ")\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn): GRU(64, 256, num_layers=3, batch_first=True, dropout=0.5)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=512, out_features=256, bias=True)\n",
      "    )\n",
      "    (rnn): GRU(260, 256, batch_first=True)\n",
      "    (out): Sequential(\n",
      "      (0): Linear(in_features=516, out_features=4, bias=False)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = in_w\n",
    "OUTPUT_DIM = cls\n",
    "ENC_EMB_DIM = in_w\n",
    "DEC_EMB_DIM = cls\n",
    "ENC_DROPOUT = 0.50\n",
    "DEC_DROPOUT = 0.50\n",
    "device = 0\n",
    "ENC_HID_DIM = 256\n",
    "DEC_HID_DIM = 256\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "cnnmodel = CNN().to(device)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "\n",
    "\n",
    "# epoch you like to use\n",
    "epoch = 179\n",
    "\n",
    "# noICF\n",
    "#SAVE_PATH_CNN = \"./params/N800/CE_3features_noicf_src/CNN_4cls_part1_layer3_std_sqe_12_Epoch_\" + str(epoch) + \".pkl\"\n",
    "#SAVE_PATH_S2S = \"./params/N800/CE_3features_noicf_src/S2S_4cls_part2_layer3_std_sqe_12_Epoch_\" + str(epoch) + \".pkl\"\n",
    "\n",
    "# with ICF\n",
    "#SAVE_PATH_CNN = \"./params/N800/CE_3features_icf_src/CNN_4cls_part1_layer3_std_sqe_12_Epoch_\" + str(epoch) + \".pkl\"\n",
    "#SAVE_PATH_S2S = \"./params/N800/CE_3features_icf_src/S2S_4cls_part2_layer3_std_sqe_12_Epoch_\" + str(epoch) + \".pkl\"\n",
    "        \n",
    "#With Focal Loss\n",
    "#SAVE_PATH_CNN = \"./params/N800/focal_gamma2_ICF_nojitter_4cls/CNN_4cls_part1_layer3_std_sqe_12_Epoch_\" + str(epoch) + \".pkl\"\n",
    "#SAVE_PATH_S2S = \"./params/N800/focal_gamma2_ICF_nojitter_4cls/S2S_4cls_part2_layer3_std_sqe_12_Epoch_\" + str(epoch) + \".pkl\"\n",
    "\n",
    "#With SQRT ICF\n",
    "\n",
    "#SAVE_PATH_CNN = \"./params/N800/CE_3features_icf_sqrt_src/CNN_4cls_part1_layer3_std_sqe_12_Epoch_\" + str(epoch) + \".pkl\"\n",
    "#SAVE_PATH_S2S = \"./params/N800/CE_3features_icf_sqrt_src/S2S_4cls_part2_layer3_std_sqe_12_Epoch_\" + str(epoch) + \".pkl\"\n",
    "\n",
    " # With real world loss\n",
    "#SAVE_PATH_CNN = \"./params/N800/CE_3features_rrc_src/CNN_4cls_part1_layer3_std_sqe_12_Epoch_\" + str(epoch ) + \".pkl\"\n",
    "#SAVE_PATH_S2S = \"./params/N800/CE_3features_rrc_src/S2S_4cls_part2_layer3_std_sqe_12_Epoch_\" + str(epoch ) + \".pkl\"\n",
    " \n",
    "# With ST constrained loss\n",
    "#SAVE_PATH_CNN = \"./params/N800/CE_3features_CE_ST_tas/CNN_4cls_part1_layer3_std_sqe_12_Epoch_\" + str(epoch) + \".pkl\"\n",
    "#SAVE_PATH_S2S = \"./params/N800/CE_3features_CE_ST_tas/S2S_4cls_part2_layer3_std_sqe_12_Epoch_\" + str(epoch) + \".pkl\"\n",
    " \n",
    "#With real World Loss and New FP Matrix \n",
    "#SAVE_PATH_CNN = \"./params/N800/CE_3features_rrc_new_fp_2_tas/CNN_4cls_part1_layer3_std_sqe_12_Epoch_\" + str(epoch ) + \".pkl\"\n",
    "#SAVE_PATH_S2S = \"./params/N800/CE_3features_rrc_new_fp_2_tas/S2S_4cls_part2_layer3_std_sqe_12_Epoch_\" + str(epoch ) + \".pkl\"    \n",
    "\n",
    "#With real World Loss, sqrt ICF and sqrt FP\n",
    "SAVE_PATH_CNN = \"./params/N800/CE_3features_icf_jitter_batch/CNN_4cls_part1_layer3_std_sqe_12_Epoch_\" + str(epoch) + \".pkl\"\n",
    "SAVE_PATH_S2S = \"./params/N800/CE_3features_icf_jitter_batch/S2S_4cls_part2_layer3_std_sqe_12_Epoch_\" + str(epoch) + \".pkl\"\n",
    "\n",
    "SAVE_PATH_CNN = \"./params/N800/CE_3features_icf_sqrt_src/CNN_4cls_part1_layer3_std_sqe_12_Epoch_\" + str(epoch) + \".pkl\"\n",
    "SAVE_PATH_S2S = \"./params/N800/CE_3features_icf_sqrt_src/S2S_4cls_part2_layer3_std_sqe_12_Epoch_\" + str(epoch) + \".pkl\"\n",
    "\n",
    "\n",
    "SAVE_PATH_CNN = \"./params/N800/CE_3features_rrc_new_fp_4_sqrt_ICF_tas/CNN_4cls_part1_layer3_std_sqe_12_Epoch_\" + str(epoch) + \".pkl\"\n",
    "SAVE_PATH_S2S = \"./params/N800/CE_3features_rrc_new_fp_4_sqrt_ICF_tas/S2S_4cls_part2_layer3_std_sqe_12_Epoch_\" + str(epoch) + \".pkl\"\n",
    "        \n",
    "\n",
    "model.load_state_dict(torch.load(SAVE_PATH_S2S))\n",
    "cnnmodel.load_state_dict(torch.load(SAVE_PATH_CNN))\n",
    "print(\"loaded!\")\n",
    "print(cnnmodel)\n",
    "print(model)\n",
    "\n",
    "\n",
    "MAX = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Mode EPOCH :  179  ACC :  0.7192613868812219\n",
      "ACC 0 wake :  0.8109109733284704  ACC 1 stage 1 :  0.7173445937108799  ACC 2 stage 2 :  0.2921931107860088  ACC 3 stage 3 :  0.6771093948293991\n",
      "84997.0     107042.0     15038.0     27579.0\n",
      "Val Mode Confusion matrix : \n",
      "[[68925. 11912.   250.  3910.]\n",
      " [12273. 76786.  6849. 11134.]\n",
      " [  508.  9146.  4394.   990.]\n",
      " [ 1756.  6899.   250. 18674.]]\n",
      "Val Score EPOCH :  179  ACC :  0.725700600027274\n",
      "ACC 0 wake :  0.8145699259973881  ACC 1 stage 1 :  0.7379906952411203  ACC 2 stage 2 :  0.28181939087644636  ACC 3 stage 3 :  0.6461438050690743\n",
      "84997.0     107042.0     15038.0     27579.0\n",
      "Val Mode Confusion matrix : \n",
      "[[69236. 12249.   185.  3327.]\n",
      " [12277. 78996.  6391.  9378.]\n",
      " [  483.  9481.  4238.   836.]\n",
      " [ 1901.  7644.   214. 17820.]]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "cnnmodel.eval()\n",
    "\n",
    "for step, (x, y, y2, lens,z ) in enumerate(val_loader):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        y3 = torch.zeros(y2.size())\n",
    "        y3[:,0:sqe,:] = y2[:,0:sqe,:]\n",
    "\n",
    "\n",
    "        score = torch.zeros(y2.size(),dtype=torch.float)\n",
    "        score[:,0:1,:] = y2[:,0:1,:]\n",
    "\n",
    "        #print(score.size())\n",
    "\n",
    "        y_mode = torch.zeros(y2.size(),dtype=torch.long)\n",
    "        y_mode[:,0:sqe,:] = y2[:,0:sqe,:]\n",
    "\n",
    "        for ii in range(sqe, 2953):\n",
    "            data_in = torch.FloatTensor(BATCH_SIZEV,sqe,in_w)\n",
    "            data_out = torch.LongTensor(BATCH_SIZEV,sqe-1,1)\n",
    "            target = torch.FloatTensor(BATCH_SIZEV, sqe, cls)\n",
    "\n",
    "            data_in[:,:,:] = x[:,ii-sqe:ii,1:]\n",
    "            data_out[:,:,:] = y[:,ii-sqe+1:ii,:]\n",
    "\n",
    "            target[:,:,:] = y3[:,ii-sqe:ii,:]\n",
    "            data_out4 = data_out.squeeze(2).reshape(BATCH_SIZEV*(sqe-1))\n",
    "            data_out = data_out.squeeze(2)\n",
    "            v_x = Variable(data_in.cuda())\n",
    "\n",
    "            trg = Variable(target.cuda())\n",
    "            t_y2 = Variable(data_out4.cuda())\n",
    "            tmp = cnnmodel(v_x)\n",
    "            outputs = model(tmp,trg, 0.0)\n",
    "\n",
    "            y3[:,ii-1,:] = outputs[:,sqe-2,:]\n",
    "            #print(outputs.size())\n",
    "            score[:,ii-sqe+1:ii,:] += outputs[:,:,:].cpu()\n",
    "\n",
    "            _, pred3 = torch.max(outputs.data, 2)\n",
    "\n",
    "            y_m = pred3.contiguous().view(200,sqe-1,1).cpu()\n",
    "\n",
    "            for kk2 in range(BATCH_SIZEV):\n",
    "                y_mode[kk2,ii-sqe+1:ii,:] += torch.zeros(outputs.size()[1],cls,dtype=torch.long).scatter_(1, y_m[kk2,:,:], 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count = 0.0\n",
    "acc = np.zeros(cls)\n",
    "total = np.zeros(cls)\n",
    "cf = np.zeros((cls,cls))\n",
    "_, y8 = torch.max(y_mode,2)\n",
    "for i in range(100):\n",
    "    l = lens[i]\n",
    "    for j in range(l):\n",
    "        cf[y[i,j,0], y8[i,j]] +=1\n",
    "        total[y[i,j,0]] +=1\n",
    "        if y8[i,j] == y[i,j,0]:\n",
    "            acc[y[i,j,0]] += 1\n",
    "            count += 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"Val Mode EPOCH : \", epoch, \" ACC : \", count/total.sum())\n",
    "print(\"ACC 0 wake : \", acc[0]/total[0], \" ACC 1 stage 1 : \", acc[1]/total[1], \" ACC 2 stage 2 : \", acc[2]/total[2], \" ACC 3 stage 3 : \", acc[3]/total[3])\n",
    "print(total[0], \"   \", total[1], \"   \", total[2] , \"   \", total[3])\n",
    "np.set_printoptions(precision=3,suppress=True)\n",
    "print(\"Val Mode Confusion matrix : \")\n",
    "print(cf)\n",
    "\n",
    "\n",
    "count = 0.0\n",
    "acc = np.zeros(cls)\n",
    "total = np.zeros(cls)\n",
    "cf = np.zeros((cls,cls))\n",
    "_, y9 = torch.max(score,2)\n",
    "for i in range(100):\n",
    "    l = lens[i]\n",
    "    for j in range(l):\n",
    "        cf[y[i,j,0], y9[i,j]] +=1\n",
    "        total[y[i,j,0]] +=1\n",
    "        if y9[i,j] == y[i,j,0]:\n",
    "            acc[y[i,j,0]] += 1\n",
    "            count += 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"Val Score EPOCH : \", epoch, \" ACC : \", count/total.sum())\n",
    "print(\"ACC 0 wake : \", acc[0]/total[0], \" ACC 1 stage 1 : \", acc[1]/total[1], \" ACC 2 stage 2 : \", acc[2]/total[2], \" ACC 3 stage 3 : \", acc[3]/total[3])\n",
    "print(total[0], \"   \", total[1], \"   \", total[2] , \"   \", total[3])\n",
    "np.set_printoptions(precision=3,suppress=True)\n",
    "print(\"Val Mode Confusion matrix : \")\n",
    "print(cf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Mode EPOCH :  179  ACC :  0.7160346851068501\n",
      "ACC 0 wake :  0.8066384950926936  ACC 1 stage 1 :  0.71029626480862  ACC 2 stage 2 :  0.2706049822064057  ACC 3 stage 3 :  0.6747686056136655\n",
      "44016.0     54107.0     7025.0     13289.0\n",
      "Val Mode Confusion matrix : \n",
      "[[35505.  6411.   173.  1927.]\n",
      " [ 6328. 38432.  3941.  5406.]\n",
      " [  254.  4467.  1901.   403.]\n",
      " [  802.  3383.   137.  8967.]]\n",
      "Val Score EPOCH :  179  ACC :  0.7227302278848671\n",
      "ACC 0 wake :  0.8100009087604507  ACC 1 stage 1 :  0.7302197497551148  ACC 2 stage 2 :  0.26234875444839856  ACC 3 stage 3 :  0.6465497780118895\n",
      "44016.0     54107.0     7025.0     13289.0\n",
      "Val Mode Confusion matrix : \n",
      "[[35653.  6617.   136.  1610.]\n",
      " [ 6426. 39510.  3645.  4526.]\n",
      " [  261.  4586.  1843.   335.]\n",
      " [  871.  3705.   121.  8592.]]\n"
     ]
    }
   ],
   "source": [
    "count = 0.0\n",
    "acc = np.zeros(cls)\n",
    "total = np.zeros(cls)\n",
    "cf = np.zeros((cls,cls))\n",
    "_, y8 = torch.max(y_mode,2)\n",
    "for i in range(100):\n",
    "    l = lens[i]\n",
    "    for j in range(l):\n",
    "        cf[y[i,j,0], y8[i,j]] +=1\n",
    "        total[y[i,j,0]] +=1\n",
    "        if y8[i,j] == y[i,j,0]:\n",
    "            acc[y[i,j,0]] += 1\n",
    "            count += 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"Val Mode EPOCH : \", epoch, \" ACC : \", count/total.sum())\n",
    "print(\"ACC 0 wake : \", acc[0]/total[0], \" ACC 1 stage 1 : \", acc[1]/total[1], \" ACC 2 stage 2 : \", acc[2]/total[2], \" ACC 3 stage 3 : \", acc[3]/total[3])\n",
    "print(total[0], \"   \", total[1], \"   \", total[2] , \"   \", total[3])\n",
    "np.set_printoptions(precision=3,suppress=True)\n",
    "print(\"Val Mode Confusion matrix : \")\n",
    "print(cf)\n",
    "\n",
    "\n",
    "count = 0.0\n",
    "acc = np.zeros(cls)\n",
    "total = np.zeros(cls)\n",
    "cf = np.zeros((cls,cls))\n",
    "_, y9 = torch.max(score,2)\n",
    "for i in range(100):\n",
    "    l = lens[i]\n",
    "    for j in range(l):\n",
    "        cf[y[i,j,0], y9[i,j]] +=1\n",
    "        total[y[i,j,0]] +=1\n",
    "        if y9[i,j] == y[i,j,0]:\n",
    "            acc[y[i,j,0]] += 1\n",
    "            count += 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"Val Score EPOCH : \", epoch, \" ACC : \", count/total.sum())\n",
    "print(\"ACC 0 wake : \", acc[0]/total[0], \" ACC 1 stage 1 : \", acc[1]/total[1], \" ACC 2 stage 2 : \", acc[2]/total[2], \" ACC 3 stage 3 : \", acc[3]/total[3])\n",
    "print(total[0], \"   \", total[1], \"   \", total[2] , \"   \", total[3])\n",
    "np.set_printoptions(precision=3,suppress=True)\n",
    "print(\"Val Mode Confusion matrix : \")\n",
    "print(cf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name=\"./prediction_file/val_4cls_cnns2s_CE_ICF_jitter_batch_score_epoch_\"+ str(epoch) + \".mat\"\n",
    "#scipy.io.savemat(save_name, {'score': score.numpy(), 'gt': y.numpy(), 'length': lens.numpy(), 'info': np.squeeze(z.numpy()), 'clock':np.squeeze(x[:,:,[0]].numpy()), 'activity':np.squeeze(x[:,:,[1]].numpy()), 'hr':x[:,:,[2,3]].numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name=\"./prediction_file2/val_4cls_cnns2s_CE_sqrt_ICF_nojitter_epoch_\" + str(epoch) +\".mat\"\n",
    "scipy.io.savemat(save_name, {'score': score.numpy(),'mode': y_mode.numpy(), 'gt': y.numpy(), 'length': lens.numpy(), 'info': np.squeeze(z.numpy()), 'clock':np.squeeze(x[:,:,[0]].numpy()), 'activity':np.squeeze(x[:,:,[1]].numpy()), 'hr':x[:,:,[2,3]].numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
