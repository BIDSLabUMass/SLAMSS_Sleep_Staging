{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import glob\n",
    "from torchvision.datasets.folder import IMG_EXTENSIONS\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 200#100\n",
    "BATCH_SIZEV = 200#560\n",
    "BATCH_SIZEV2 = 600\n",
    "EPOCH = 500\n",
    "w = 3\n",
    "in_w = 3\n",
    "#\n",
    "sqe = 12\n",
    "# # of class\n",
    "cls = 4\n",
    "kk= 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "    \n",
    "        self.cnn = nn.Sequential( \n",
    "        nn.Conv1d(in_w, 64, 9, stride=1, padding=4),\n",
    "        nn.LeakyReLU(inplace=False),\n",
    "        nn.Conv1d(64, 64, 9, stride=1, padding=4),\n",
    "        nn.LeakyReLU(inplace=False),\n",
    "        nn.Conv1d(64, 64, 9, stride=1, padding=4),\n",
    "        #nn.LeakyReLU(inplace=False), #put it back 2020 706\n",
    "        )\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        \n",
    "        #embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src sent len, batch size, emb dim]\n",
    "\n",
    "        src1 = src.permute(0, 2, 1)\n",
    "        #print src1.size()\n",
    "        src2 = self.cnn(src1)\n",
    "        #print src2.size()\n",
    "        src3 = src2.permute(0, 2, 1)\n",
    "        #print src3.size()\n",
    "\n",
    "    \n",
    "    \n",
    "        return src3\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 1) + dec_hid_dim, dec_hid_dim, bias=True)\n",
    "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        \n",
    "\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        #repeat encoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        #encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src sent len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        #print energy\n",
    "        #energy = [batch size, src sent len, dec hid dim]\n",
    "        \n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        \n",
    "        #energy = [batch size, dec hid dim, src sent len]\n",
    "        \n",
    "        #v = [dec hid dim]\n",
    "        \n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
    "        \n",
    "        #v = [batch size, 1, dec hid dim]\n",
    "     \n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        \n",
    "        #attention= [batch size, src len]\n",
    "\n",
    "        \n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "        #ORINAL 1 # 5\n",
    "        self.rnn = nn.GRU(64, enc_hid_dim, 3, bidirectional = False, dropout=0.5, batch_first = True)\n",
    "        \n",
    "        self.fc = nn.Sequential( \n",
    "            nn.Linear(enc_hid_dim * 1, dec_hid_dim, bias=True),\n",
    "            #nn.BatchNorm1d(dec_hid_dim),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "     \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        \n",
    "\n",
    "        outputs, hidden = self.rnn(src)\n",
    "  \n",
    "        #hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        hidden = self.fc(hidden[-1,:,:])\n",
    "        #hidden = self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "\n",
    "        #outputs = [src sent len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        #print outputs\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        #self.dropout = dropout\n",
    "        self.attention = attention\n",
    "        \n",
    "        #self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 1) + emb_dim, dec_hid_dim,batch_first = True, dropout=0.0)\n",
    "        \n",
    "        self.out = nn.Sequential( \n",
    "\n",
    "            nn.Linear((enc_hid_dim * 1) + dec_hid_dim + emb_dim, output_dim, bias=False),\n",
    "            #nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs, trans):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        #print a        \n",
    "        #a = [batch size, src len]\n",
    "    \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        encoder_outputs = encoder_outputs.permute(0, 1, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "\n",
    "        rnn_input = torch.cat((input, weighted), dim = 2)\n",
    "        rnn_input = rnn_input.permute(1, 0, 2)\n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "        #print hidden.size()\n",
    "\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        #print output \n",
    "        #output = [sent len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "\n",
    "        #assert (output == hidden).all()\n",
    "        \n",
    "        #input = input.squeeze(0)\n",
    "        output = output.squeeze(1)\n",
    "        output = output.unsqueeze(0)\n",
    "        #weighted = weighted.squeeze(0)\n",
    "        #print(output.size())\n",
    "        #print(weighted.size())\n",
    "        #print(input.size())\n",
    "        #print(trans.size())\n",
    "        #trans = trans.unsqueeze(0)\n",
    "        #output = self.out(torch.cat((output, weighted, input, trans), dim = 2))\n",
    "        output = self.out(torch.cat((output, weighted, input), dim = 2))\n",
    "        #output = [bsz, output dim]\n",
    "        \n",
    "        return output, hidden.squeeze(0)\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device = 0):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        tn = scipy.io.loadmat(\"./transition.mat\")\n",
    "        trans = tn['transition_train_raw']\n",
    "        self.trans = Variable(torch.FloatTensor(trans).cuda())\n",
    "        #print(trans)\n",
    "        #assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "        #    \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        #assert encoder.n_layers == decoder.n_layers, \\\n",
    "        #    \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        max_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, max_len-1, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        #print hidden.size()\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[:,0]\n",
    "        _, ind = torch.max(input,1)\n",
    "        trans_bat = torch.zeros(batch_size, 5).to(self.device)\n",
    "        for i in range(batch_size):\n",
    "            trans_bat[i,:] = self.trans[ind[i],:]\n",
    "        #print(input.size())\n",
    "        #print(input2.size())\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs, trans_bat)\n",
    "\n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "   \n",
    "            if output.size()[0] == 1:\n",
    "                outputs[:,t-1,:] = output[:,:]\n",
    "            else:\t\n",
    "                outputs[:,t-1,:] = output[:,0,:]\n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            #need to chekc again   tzuan\n",
    "            top1 = output.argmax(1)\n",
    "            top1 = top1.view(-1,1).type(torch.cuda.FloatTensor)\n",
    "            #print \"Tp: \", top1.size()\n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[:,t] if teacher_force else output.squeeze(0)\n",
    "        \n",
    "\n",
    "        return outputs\n",
    "\n",
    "class features_dataset(Data.Dataset):\n",
    "\n",
    "    def __init__(self, mode, p):\n",
    "\n",
    "        self.mode = mode\n",
    "        self.data = glob.glob(p)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "\n",
    "        if self.mode == 0:\n",
    "            data = scipy.io.loadmat(self.data[index])\n",
    "            x = np.squeeze(data['sqe_x'])[:800,:]\n",
    "            y = data['sqe_y'][:800,:]\n",
    "        elif self.mode == 1:\n",
    "            data = scipy.io.loadmat(self.data[index])\n",
    "            x = np.squeeze(data['sqe_x'])\n",
    "            y = data['sqe_y']\n",
    "            up_lim = x.shape[0] - 700\n",
    "            ind = random.randint(sqe, up_lim)\n",
    "            tmp_x = x[(ind-sqe):ind+700,:]\n",
    "            tmp_y = y[(ind-sqe):ind+700,:]\n",
    "            x = tmp_x#.reshape((sqe+50, w)) + np.random.normal(0,0.000001, size=(50+sqe, w))\n",
    "            y = tmp_y\n",
    "\n",
    "        else:\n",
    "            data = scipy.io.loadmat(self.data[index])\n",
    "            x = np.squeeze(data['sqe_x'])\n",
    "            y = data['sqe_y']\n",
    "            tem_x = np.zeros((2953,3))\n",
    "            tem_y = np.zeros((2953,1))\n",
    "            tem_x[:x.shape[0],:] = x[:,[1,6,7]]\n",
    "            tem_y[:y.shape[0],:] = y[:,:]\n",
    "            x1 = torch.FloatTensor(tem_x) \n",
    "            y1 = torch.LongTensor(tem_y)\n",
    "            y2 = torch.zeros(x1.size()[0],cls).scatter_(dim=1, index=y1, src=torch.tensor(1.0))\n",
    "            \n",
    "            return x1, y1, y2, x.shape[0]\n",
    "        #x = self.input_x[index].reshape((h, w))\n",
    "        #y = self.input_y[index]\t\t\t\n",
    "        #x = x[:,[0, 1, 6]]\t\n",
    "        x = x[:,[1,6,7]]\n",
    "        x1 = torch.FloatTensor(x) \n",
    "        y1 = torch.LongTensor(y)#.unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        #print(y1.size(), \"                    \"  ,x1.size())\n",
    "\n",
    "        #print(y1.size())\n",
    "        y2 = torch.zeros(x1.size()[0],cls).scatter_(dim=1, index=y1, src=torch.tensor(1.0))\n",
    "        #print y2.size()\n",
    "        #quit()\n",
    "\n",
    "        return x1, y1, y2\n",
    "    def __len__(self):\n",
    "        if self.mode == 1:\n",
    "            return(len(self.data))\n",
    "        else:\n",
    "            return(len(self.data))\n",
    "            #return(len(self.x))\n",
    "    def getName(self):\n",
    "        return self.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_d = \"./data_focal5_t_noaug/*.mat\"\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    features_dataset(mode = 1, p=path_d),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "path_d = \"./data_focal5_t_noaug/*.mat\"\n",
    "train_loader2 = torch.utils.data.DataLoader(\n",
    "    features_dataset(mode = 0, p=path_d),\n",
    "    batch_size = BATCH_SIZEV2,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "path_d = \"./data_focal5_v/*.mat\"\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    features_dataset(mode = 2, p=path_d),\n",
    "    batch_size = BATCH_SIZEV,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = in_w\n",
    "OUTPUT_DIM = cls\n",
    "ENC_EMB_DIM = in_w\n",
    "DEC_EMB_DIM = cls\n",
    "ENC_DROPOUT = 0.50\n",
    "DEC_DROPOUT = 0.50\n",
    "device = 0\n",
    "ENC_HID_DIM = 256\n",
    "DEC_HID_DIM = 256\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "cnnmodel = CNN().to(device)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "#model.load_state_dict(torch.load(SAVE_PATH))\n",
    "SAVE_PATH_CNN = \"./params/616/CNN_4cls_part1.pkl\"\n",
    "SAVE_PATH_S2S = \"./params/616/S2S_4cls_part2.pkl\"\n",
    "SAVE_PATH_CNN = \"./params/628/CNN_4cls_part1_layer2_std_sqe_15.pkl\"\n",
    "SAVE_PATH_S2S = \"./params/628/S2S_4cls_part2_layer2_std_sqe_15.pkl\"\n",
    "#model.load_state_dict(torch.load(SAVE_PATH_S2S))\n",
    "#cnnmodel.load_state_dict(torch.load(SAVE_PATH_CNN))\n",
    "#print(\"loaded!\")\n",
    "print(cnnmodel)\n",
    "print(model)\n",
    "\n",
    "\n",
    "MAX = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCCLoss(nn.Module):\n",
    "    def __init__(self, fn_weights=None, fp_weights=None):\n",
    "        super(RCCLoss,self).__init__()\n",
    "        #anti_eye = np.ones((k,k)) - np.eye(k)\n",
    "        self.full_fn_weights = torch.Tensor(fn_weights)\n",
    "        self.full_fp_weights = torch.Tensor(fp_weights)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        #print(x.size()[0])\n",
    "        #print(y.size())\n",
    "        #print(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        yi = torch.cuda.LongTensor(y.unsqueeze(1))\n",
    "        #print(yi)\n",
    "        y1 = torch.zeros(x.size()[0],cls).cuda().scatter_(dim=1, index=yi, src=torch.tensor(1.0))\n",
    "        #print(y1)\n",
    "        if y1.type() != x.data.type():\n",
    "            y1 = y1.type_as(x.data)\n",
    "        eps=1e-5\n",
    "        \n",
    "        x = torch.clamp(x, eps, 1. - eps) \n",
    "        \n",
    "        logs = torch.log(x)\n",
    "        \n",
    "        logs_1_sub = torch.log(1.-x) # shape (m, k), dense. 0 is good. \n",
    "        #logs =  F.log_softmax(x)\n",
    "        \n",
    "        #logs_1_sub =  F.log_softmax(1.-x) # shape (m, k), dense. 0 is good. \n",
    "        #print(logs_1_sub)\n",
    "        if self.full_fn_weights.type() != x.data.type():\n",
    "            self.full_fn_weights = self.full_fn_weights.type_as(x.data)\n",
    "\n",
    "        if self.full_fp_weights.type() != x.data.type():\n",
    "            self.full_fp_weights = self.full_fp_weights.type_as(x.data)\n",
    "        \n",
    "        m_full_fn_weights = torch.mm(y1, self.full_fn_weights) # (m,k) . (k, k)\n",
    "        \n",
    "        m_full_fp_weights = torch.mm(y1, self.full_fp_weights) # (m,k) . (k, k)\n",
    "        #print(y1.type())\n",
    "        #print(self.full_fn_weights.type())\n",
    "        #print(m_full_fp_weights.type())\n",
    "        loss = -(m_full_fn_weights * logs + m_full_fp_weights * logs_1_sub).sum()\n",
    "        \n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "icf_sqrt = [\n",
    "    1.6647,\n",
    "    1.4874,\n",
    "    3.8379,\n",
    "    2.8959]\n",
    "fn_weights =[\n",
    " [1.6647, 0.00, 0.00, 0.00],\n",
    " [0.00, 1.4874, 0.00, 0.00],\n",
    " [0.00, 0.00, 3.8379, 0.00],\n",
    " [0.00, 0.00, 0.00, 2.8959]]\n",
    "\n",
    "\n",
    "\n",
    "fp_weights=[\n",
    " [0.00, 0.79, 0.17, 3.00],\n",
    " [0.79, 0.00, 12.00, 3.80],\n",
    " [0.17, 7.00, 0.00, 1.80],\n",
    " [3.00, 3.80, 1.80, 0.00]]\n",
    "\n",
    "fp_weights2=[\n",
    " [0.00, 1.25, 5.88, 0.33],\n",
    " [0.79, 0.00, 0.14, 0.25],\n",
    " [0.17, 7.00, 0.00, 0.50],\n",
    " [3.00, 3.80, 1.80, 0.00]]\n",
    "\n",
    "fp_weights3=[\n",
    " [0.00, 0.79, 0.17, 3],\n",
    " [1.25, 0.00, 7, 3.8],\n",
    " [5.88, 0.14, 0.00, 0.18],\n",
    " [0.33, 0.25, 0.5, 0.00]]\n",
    "\n",
    "fp_weights4= [\n",
    " [0.00, 1.25**0.5, 5.88**0.5, 0.33**0.5],\n",
    " [0.79**0.5, 0.00, 0.14**0.5, 0.25**0.5],\n",
    " [0.17**0.5, 7.00**0.5, 0.00, 0.50**0.5],\n",
    " [3.00**0.5, 3.80**0.5, 1.80**0.5, 0.00]]\n",
    "\n",
    "print(fp_weights4)\n",
    "loss_func = RCCLoss(fn_weights=fn_weights, fp_weights=fp_weights4)\n",
    " \n",
    "LR = 0.00015\n",
    "print(loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_t = np.zeros(EPOCH)\n",
    "loss_v = np.zeros(EPOCH)\n",
    "acc_t = np.zeros(EPOCH)\n",
    "acc_v = np.zeros(EPOCH)\n",
    "for epoch in range(EPOCH):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    optimizer2 = torch.optim.Adam(cnnmodel.parameters(), lr=LR)\n",
    "    model.train()\n",
    "    cnnmodel.train()\n",
    "    #training\n",
    "\n",
    "    for step, (x, y, y2) in enumerate(train_loader):\n",
    "\n",
    "        y3 = torch.zeros(y2.size())\n",
    "        #y3[:,0:2,:] = y2[:,0:2,:]\n",
    "        y3 = y2\n",
    "\n",
    "        for ii in range(sqe, 700+sqe):\n",
    "            #ii = \n",
    "            data_in = torch.FloatTensor(BATCH_SIZE, sqe, in_w)\n",
    "            data_out = torch.LongTensor(BATCH_SIZE, 1,1)\n",
    "            data_out3 = torch.LongTensor(BATCH_SIZE, sqe-1, 1)\n",
    "            target = torch.FloatTensor(BATCH_SIZE, sqe, cls)\n",
    "            target2 = torch.FloatTensor(BATCH_SIZE, sqe-1, cls)\n",
    "\n",
    "            data_in[:,:,:] = x[:,ii-sqe:ii,:]\n",
    "    \n",
    "            data_out3[:,:,:] = y[:,ii-sqe+1:ii,:]\n",
    "            data_out4 = data_out3.squeeze(2).reshape(BATCH_SIZE*(sqe-1))\n",
    "            data_out5 = data_out3.squeeze(2).reshape(BATCH_SIZE,(sqe-1)).permute(1,0)\n",
    "\n",
    "            target[:,:,:] = y3[:,ii-sqe:ii,:] \n",
    "            target2[:,:,:] = y2[:,ii-sqe+1:ii,:] \n",
    "            t_x = Variable(data_in.cuda())\n",
    "            t_y2 = Variable(data_out4.cuda())\n",
    "            t_y3 = Variable(data_out5.cuda())\n",
    "            trg = Variable(target.cuda())\n",
    "            trg2 = Variable(target2.cuda())\n",
    "            tmp = cnnmodel(t_x)\n",
    "            outputs = model(tmp,trg, 0.0)\n",
    "\n",
    "            _, pred = torch.max(outputs[:,sqe-2,:].contiguous().view(-1,cls).data, 1)\n",
    "            y_3 = torch.zeros(BATCH_SIZE,cls).scatter_(dim=1, index=pred.cpu().unsqueeze(1).data, src=torch.tensor(1.0))\n",
    "\n",
    "            y3[:,ii-1,:] = outputs[:,sqe-2,:]\n",
    "\n",
    "            \n",
    "             \n",
    "            #print(outputs)    \n",
    "            loss = loss_func(outputs.contiguous().view(-1,cls), t_y2)\n",
    "            #print(loss)\n",
    "            optimizer.zero_grad()   # clear gradients for next train\n",
    "            optimizer2.zero_grad()\n",
    "            loss.backward()         # backpropagation, compute gradients \n",
    "\n",
    "            optimizer.step()        # apply gradients\n",
    "            optimizer2.step()\n",
    "    if (epoch + 1)%1 ==0:\n",
    "\n",
    "        print(\"Training done!\")\n",
    "        model.eval()\n",
    "        cnnmodel.eval()\n",
    "        mse = np.zeros(BATCH_SIZEV2)\n",
    "        pmse = np.zeros(BATCH_SIZEV2)\n",
    "        count = 0.0\n",
    "        acc = np.zeros(cls)\n",
    "        total = np.zeros(cls)\n",
    "        cf = np.zeros((cls,cls))\n",
    "\n",
    "        for step, (x, y, y2) in enumerate(train_loader2):\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                y3 = torch.zeros(y2.size())\n",
    "                y3[:,0:sqe,:] = y2[:,0:sqe,:]\n",
    "                y5  = torch.zeros(y.size(),dtype=torch.long)\n",
    "                y5[:,0:1,:] = y[:,0:1,:]\n",
    "                y_mode = torch.zeros(y2.size(),dtype=torch.long)\n",
    "                y_mode[:,0:sqe,:] = y2[:,0:sqe,:]\n",
    "                for ii in range(sqe, 801):\n",
    "\n",
    "                    data_in = torch.FloatTensor(BATCH_SIZEV2,sqe,in_w)\n",
    "                    data_out = torch.LongTensor(BATCH_SIZEV2,sqe-1,1)\n",
    "                    target = torch.FloatTensor(BATCH_SIZEV2, sqe, cls)\n",
    "                    data_in[:,:,:] = x[:,ii-sqe:ii,:]\n",
    "                    data_out[:,:,:] = y[:,ii-sqe+1:ii,:]\n",
    "\n",
    "                    target[:,:,:] = y3[:,ii-sqe:ii,:]\n",
    "                    data_out4 = data_out.squeeze(2).reshape(BATCH_SIZEV2*(sqe-1))\n",
    "                    data_out = data_out.squeeze(2)\n",
    "\n",
    "                    v_x = Variable(data_in.cuda())\n",
    "                    trg = Variable(target.cuda())\n",
    "                    t_y2 = Variable(data_out4.cuda())\n",
    "                    tmp = cnnmodel(v_x)\n",
    "                    outputs = model(tmp,trg, 0.0)\n",
    "                    #loss = loss_func(outputs.contiguous().view(-1,cls), t_y2)\n",
    "                    #loss_t[epoch] = loss \n",
    "                    y3[:,ii-1,:] = outputs[:,sqe-2,:]\n",
    "                    _, pred2 = torch.max(outputs[:,sqe-2,:].contiguous().view(-1,cls).data, 1)\n",
    "                    plabel = pred2.cpu().data\n",
    "                    _, pred3 = torch.max(outputs.data, 2)\n",
    "\n",
    "                    tt = torch.cat((y5[:,ii-sqe+1:ii,:],pred3.contiguous().view(BATCH_SIZEV2,sqe-1,1).cpu()),2)\n",
    "                    tt3, tt2 = torch.max(tt,2)\n",
    "                    y5[:,ii-sqe+1:ii,0] = tt3\n",
    "                    y_m = pred3.contiguous().view(BATCH_SIZEV2,sqe-1,1).cpu()\n",
    "                    for kk in range(BATCH_SIZEV):\n",
    "                        y_mode[kk,ii-sqe+1:ii,:] += torch.zeros(outputs.size()[1],cls,dtype=torch.long).scatter_(dim=1, index=y_m[kk,:,:], src=torch.tensor(1.0))\n",
    "\n",
    "                    for tt in range(BATCH_SIZEV2):\n",
    "                        cf[data_out4[(tt+1)*(sqe-1)-1], plabel[tt]] +=1\n",
    "                        total[data_out4[(tt+1)*(sqe-1)-1]] +=1\n",
    "\n",
    "                        if plabel[tt] == data_out4[(tt+1)*(sqe-1)-1]:\n",
    "                            acc[data_out4[(tt+1)*(sqe-1)-1]] += 1\n",
    "                            count += 1\n",
    "        acc_t[epoch] =count/total.sum()\n",
    "        print(\"Train EPOCH : \", epoch + 1, \" ACC : \", count/total.sum(), \"    LR: \", LR)\n",
    "        print(\"ACC 0 wake : \", acc[0]/total[0], \" ACC 1 stage 1 : \", acc[1]/total[1], \" ACC 2 stage 2 : \", acc[2]/total[2], \" ACC 3 stage 3 : \", acc[3]/total[3])\n",
    "        #print(\"Train Loss: \", loss_t[epoch])\n",
    "        print(total[0], \"   \", total[1], \"   \", total[2] , \"   \", total[3])\n",
    "        np.set_printoptions(precision=3,suppress=True)\n",
    "        print(\"Train Confusion matrix : \")\n",
    "        print(cf)\n",
    "\n",
    "        mse = np.zeros(BATCH_SIZEV)\n",
    "        pmse = np.zeros(BATCH_SIZEV)\n",
    "        count = 0.0\n",
    "        acc = np.zeros(cls)\n",
    "        total = np.zeros(cls)\n",
    "        cf = np.zeros((cls,cls))\n",
    "\n",
    "        for step, (x, y, y2, lens) in enumerate(val_loader):\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "\n",
    "                #mem = torch.FloatTensor(BATCH_SIZEV, 600, cls)\n",
    "                y3 = torch.zeros(y2.size())\n",
    "                y3[:,0:sqe,:] = y2[:,0:sqe,:]\n",
    "                y5  = torch.zeros(y.size(),dtype=torch.long)\n",
    "                y5[:,0:1,:] = y[:,0:1,:]\n",
    "                y6  = torch.zeros(y.size(),dtype=torch.long)\n",
    "                y6[:,0:sqe,:] = y[:,0:sqe,:]\n",
    "                y_mode = torch.zeros(y2.size(),dtype=torch.long)\n",
    "                y_mode[:,0:sqe,:] = y2[:,0:sqe,:]\n",
    "\n",
    "                \n",
    "                for ii in range(sqe, 2953):\n",
    "                    data_in = torch.FloatTensor(BATCH_SIZEV,sqe,in_w)\n",
    "                    data_out = torch.LongTensor(BATCH_SIZEV,sqe-1,1)\n",
    "                    target = torch.FloatTensor(BATCH_SIZEV, sqe, cls)\n",
    "\n",
    "                    data_in[:,:,:] = x[:,ii-sqe:ii,:]\n",
    "                    data_out[:,:,:] = y[:,ii-sqe+1:ii,:]\n",
    "\n",
    "                    target[:,:,:] = y3[:,ii-sqe:ii,:]\n",
    "                    data_out4 = data_out.squeeze(2).reshape(BATCH_SIZEV*(sqe-1))\n",
    "                    data_out = data_out.squeeze(2)\n",
    "                    v_x = Variable(data_in.cuda())\n",
    "                    #v_y = Variable(data_out.cuda())\n",
    "                    trg = Variable(target.cuda())\n",
    "                    t_y2 = Variable(data_out4.cuda())\n",
    "                    tmp = cnnmodel(v_x)\n",
    "                    outputs = model(tmp,trg, 0.0)\n",
    "                    #loss = loss_func(outputs.contiguous().view(-1,cls), t_y2)\n",
    "                    #loss_v[epoch] = loss \n",
    "                    y3[:,ii-1,:] = outputs[:,sqe-2,:]\n",
    "\n",
    "                    _, pred2 = torch.max(outputs[:,sqe-2,:].contiguous().view(-1,cls).data, 1)\n",
    "                    plabel = pred2.cpu().data\n",
    "\n",
    "                    _, pred3 = torch.max(outputs.data, 2)\n",
    "\n",
    "                    tt = torch.cat((y5[:,ii-sqe+1:ii,:],pred3.contiguous().view(BATCH_SIZEV,sqe-1,1).cpu()),2)\n",
    "                    tt3, tt2 = torch.max(tt,2)\n",
    "                    y5[:,ii-sqe+1:ii,0] = tt3                    \n",
    "                    y_m = pred3.contiguous().view(BATCH_SIZEV,sqe-1,1).cpu()\n",
    "                    for kk2 in range(BATCH_SIZEV):\n",
    "                        y_mode[kk2,ii-sqe+1:ii,:] += torch.zeros(outputs.size()[1],cls,dtype=torch.long).scatter_(dim=1, index=y_m[kk2,:,:], src=torch.tensor(1.0))\n",
    "\n",
    "                    for tt in range(BATCH_SIZEV):\n",
    "\n",
    "                        cf[data_out4[(tt+1)*(sqe-1)-1], plabel[tt]] +=1\n",
    "\n",
    "                        total[data_out4[(tt+1)*(sqe-1)-1]] +=1\n",
    "                        if plabel[tt] == data_out4[(tt+1)*(sqe-1)-1]:\n",
    "                            acc[data_out4[(tt+1)*(sqe-1)-1]] += 1\n",
    "                            count += 1\n",
    "\n",
    "\n",
    "\n",
    "        mse = np.zeros(BATCH_SIZEV)\n",
    "        pmse = np.zeros(BATCH_SIZEV)\n",
    "        count = 0.0\n",
    "        acc = np.zeros(cls)\n",
    "        total = np.zeros(cls)\n",
    "        cf = np.zeros((cls,cls))\n",
    "        _, y8 = torch.max(y_mode,2)\n",
    "        for i in range(BATCH_SIZEV):\n",
    "            l = lens[i]\n",
    "            for j in range(l):\n",
    "                cf[y[i,j,0], y8[i,j]] +=1\n",
    "                total[y[i,j,0]] +=1\n",
    "                if y8[i,j] == y[i,j,0]:\n",
    "                    acc[y[i,j,0]] += 1\n",
    "                    count += 1\n",
    "        acc_v[epoch] = count/total.sum()                   \n",
    "        print(\"Val Mode EPOCH : \", epoch + 1, \" ACC : \", count/total.sum(), \"    LR: \", LR)\n",
    "        print(\"ACC Wake : \", acc[0]/total[0], \" ACC Light : \", acc[1]/total[1], \" ACC Deep : \", acc[2]/total[2], \" ACC REM : \", acc[3]/total[3])\n",
    "        print(\"Val loss: \", loss_v[epoch])\n",
    "        print(total[0], \"   \", total[1], \"   \", total[2] , \"   \", total[3])\n",
    "\n",
    "        np.set_printoptions(precision=3,suppress=True)\n",
    "        print(\"Val Mode Confusion matrix : \")\n",
    "        print(cf)\n",
    "        # saving models\n",
    "        SAVE_PATH_CNN = \"./params/N800/CE_3features_rrc_new_fp_4_sqrt_ICF_tas/CNN_4cls_part1_layer3_std_sqe_12_Epoch_\" + str(epoch + 1) + \".pkl\"\n",
    "        SAVE_PATH_S2S = \"./params/N800/CE_3features_rrc_new_fp_4_sqrt_ICF_tas/S2S_4cls_part2_layer3_std_sqe_12_Epoch_\" + str(epoch + 1) + \".pkl\"\n",
    "        torch.save(cnnmodel.state_dict(), SAVE_PATH_CNN)\n",
    "        torch.save(model.state_dict(), SAVE_PATH_S2S)\n",
    "        print(\"Model saved!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
